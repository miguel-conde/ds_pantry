# Modeltime Workflow for Panel Data using a Global XGBOOST Model
#
# https://www.business-science.io/code-tools/2021/07/19/modeltime-panel-data.html
#
# What are Panel Data and Global Models?
#
# In it’s simplest form, Panel Data is a time series dataset that has more than 
# one series. Each time series is stacked row-wise (on-top) of each other.
#
# Traditional modeling techniques like ARIMA can only be used on one time series 
# at a time. The widely accepted forecasting approach is to iterate through each 
# time series producing a unique model and forecast for each time series i
# dentifier. The downside with this approach is that it’s expensive when you 
# have many time series. Think of the number of products in a database. As the 
# number of time series approaches the range of 1000-10,000, the iterative 
# approach becomes unscalable as for-loops run endlessly and errors can grind 
# your analysis to a hault.
#
# Global Models are alternatives to the iterative approach. A Global Model is a 
# single model that forecasts all time series at once. Global Models are highly 
# scalable, which solves the problem of 1-10,000 time series. An example is an 
# XGBoost Model, which can determine relationships for all 1000 time series 
# panels with a single model. This is great: No For-Loops!

library(tidymodels)
library(modeltime)
library(tidyverse)
library(timetk)


# 1 - Collect data --------------------------------------------------------

# The dataset consists of 1001 observations of revenue generated by a 
# store-department combination on any given week. It contains:
# 
# - 7 Time Series Groups denoted by the “ID” column
# - The data is structured in Panel Data format
# - The time series groups will be modeled with a single Global Model

data <- walmart_sales_weekly %>% 
  select(id, Date, Weekly_Sales) %>%
  set_names(c("ID", "date", "value"))

data

# 2 - Visualize the Data --------------------------------------------------

# Most of the series have yearly seasonality and long-term trends.

data %>%
  group_by(ID) %>%
  plot_time_series(
    .date_var    = date, 
    .value       = value,
    .facet_ncol  = 3,
    .interactive = FALSE
  )


# 3 - Train/Test Splitting ------------------------------------------------

# We can split the data into training and testing sets using time_series_split().
# We’ll investigate the last 3-months of the year to test a global model on a 
# 3-month forecast. The message on overlapping dates is to let us know that 
# multiple time series are being processed using the last 3-month window for 
# testing.

splits <- data %>% time_series_split(assess = "3 months", cumulative = TRUE)
splits


# 4 - Feature Engineering (Recipe) ----------------------------------------


rec_obj <- recipe(value ~ ., training(splits)) %>%
  step_mutate(ID = droplevels(ID)) %>%
  step_timeseries_signature(date) %>%
  step_rm(date) %>%
  step_zv(all_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE)

summary(prep(rec_obj))


# 5 - Machine Learning ----------------------------------------------------

# Workflow
wflw_xgb <- workflow() %>%
  add_model(
    boost_tree() %>% set_engine("xgboost")
  ) %>%
  add_recipe(rec_obj) %>%
  fit(training(splits))

wflw_xgb

# 6 - Modeltime Workflow --------------------------------------------------


# Create a Modeltime Table ------------------------------------------------

model_tbl <- modeltime_table(
  wflw_xgb
)

model_tbl

# Calibrate by ID ---------------------------------------------------------

calib_tbl <- model_tbl %>%
  modeltime_calibrate(
    new_data = testing(splits), 
    id       = "ID"
  )

calib_tbl


# Measure Accuracy --------------------------------------------------------

# Global Accuracy

calib_tbl %>% 
  modeltime_accuracy(acc_by_id = FALSE) %>% 
  table_modeltime_accuracy(.interactive = FALSE)

# Local Accuracy

calib_tbl %>% 
  modeltime_accuracy(acc_by_id = TRUE) %>% 
  table_modeltime_accuracy(.interactive = FALSE)


# Forecast the Data -------------------------------------------------------

calib_tbl %>%
  modeltime_forecast(
    new_data    = testing(splits),
    actual_data = bind_rows(training(splits), testing(splits)),
    conf_by_id  = TRUE
  ) %>%
  group_by(ID) %>%
  plot_modeltime_forecast(
    .facet_ncol  = 3,
    .interactive = FALSE
  )
