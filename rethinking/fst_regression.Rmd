---
title: "fst_regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data

```{r}
library(tidyverse)
library(rethinking)

data(Howell1)

d <- Howell1

head(d)
```

```{r}
str(d)
```

```{r}
d2 <- d %>% filter(age >= 18)
```

```{r}
plot(density(d2$height))
```

# Primer modelo

Posible modelo:

$$
h_i \sim Normal(\mu, \sigma) \\
\mu \sim Normal(183, 30) \\
\sigma \sim Unif(0, 50)
$$
¿Qué pinta tienen las prior?

```{r}
curve(dnorm(x, 183, 30), from = 100, to = 250, main = "Sample prior $mu")
```
```{r}
curve(dunif(x, 0, 50), from = -10, to = 60, main = "Sample prior $sigma")
```
**PRIOR PREDICTIVE**:

```{r}
sample_mu <- rnorm(1e4, 183, 30)
sample_sigma <- runif(1e4, 0, 50)

predictive_prior_sample_h <- rnorm(1e4, sample_mu, sample_sigma)
```

```{r}
plot(density(predictive_prior_sample_h), main = "h - Predictive prior sample")
```

La distribución *posterior* será ahora:

$$
\Pr(\mu, \sigma | h) = \frac{\Pi_i\text{Normal}(h_i|\mu, \sigma)}
{\int \int \left[\Pi_i\text{Normal}(h_i|\mu, \sigma)\right] \text{Normal}(183, 30) \text{Unif}(0, 50) d\mu d\sigma} \text{Normal}(183, 30) \text{Unif}(0, 50)
$$
## Posterior mediante grid

```{r}
mu.list    <- seq(from = 150, to = 160, length.out = 100)
sigma.list <- seq(from = 7, to = 9, length.out = 100)

post <- expand.grid(mu = mu.list, sigma = sigma.list)

post$LL <- sapply(1:nrow(post), function(i) sum(dnorm(d2$height, 
                                                      post$mu[i], 
                                                      post$sigma[i], 
                                                      log = TRUE)))
post$prod <- post$LL + 
  dnorm(post$mu, 183, 30, TRUE) +
  dunif(post$sigma, 0, 50, TRUE)

# Finally, the obstacle for getting back on the probability scale is that 
# rounding error is always a threat when moving from log-probability to 
# probability. If you use the obvious approach,like exp(post$prod), you’ll get a 
# vector full of zeros, which isn’t very helpful.This is a result of R’s rounding 
# very small probabilities to zero. Remember, in large samples, all unique samples 
# are unlikely. This is why you have to work with log-probability. The code in 
# the box dodges this problem by scaling all of the log-products by the maximum 
# log-product. As a result, the values in post$prob are not all zero, but they 
# also aren’t exactly probabilities. Instead they are relative posterior 
# probabilities. But that’s good enough for what we wish to do with these values.

post$prob <- exp(post$prod - max(post$prod))
```

```{r}
contour_xyz(post$mu, post$sigma, post$prob, xlab = "mu", ylab = "sigma")
```

```{r}
image_xyz(post$mu, post$sigma, post$prob, xlab = "mu", ylab = "sigma")
```
## Muestreo de la *posterior*

```{r}
sample.rows  <- sample(1:nrow(post), size = 1e4, replace = TRUE, prob = post$prob)
sample.mu    <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]
```

```{r}
plot(sample.mu, sample.sigma, cex=0.5, pch=16, col = col.alpha(rangi2, 0.1))
```

Formas de las distribuciones *posterior* **marginales**:

```{r}
dens(sample_mu)
```
```{r}
dens(sample.sigma)
```

```{r}
PI(sample_mu)
```

```{r}
PI(sample_sigma)
```

## Muestreo de la *posterior* con aproximación cuadrática

Estimamos la *posterior* mediante aproximación cuadrática:

```{r}
flist <- alist(
  height ~ dnorm(mu,sigma),
  mu ~ dnorm(183, 30),
  sigma ~ dunif(0, 50)
)
```

```{r}
m4.1 <- quap(flist, data = d2)
```

```{r}
precis(m4.1)
```

Matriz varianza - covarianza (ya que estamos trabajando con una distribución
gaussiana bidimensional):

```{r}
vcov(m4.1)
```

Vector de varianzas:

```{r}
diag( vcov(m4.1))
```

Matriz de corrleaciones:

```{r}
cov2cor( vcov(m4.1))
```
Ahora el muestreo:

```{r}
post <- extract.samples(m4.1, n = 1e4)
```


```{r}
head(post)
```

```{r}
precis(post)
```
```{r}
plot(post)
```

Este muestreo es como hacer:

```{r}
MASS::mvrnorm(n = 1e4, mu = coef(m4.1), Sigma = vcov(m4.1)) %>% head()
```

# Predicción Lineal

```{r}
plot(height ~weight, d2)
```
The strategy is to make the parameter for the mean of
a Gaussian distribution, $\mu$, into a linear function of th epredictor variable and other,new
parameters that we invent.

## Modelo

$$
h_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta (x_i - \bar{x}) \\
\alpha \sim \text{Normal}(183, 30) \\
\beta \sim \text{Normal}(0, 10) \\
\sigma \sim \text{Normal}(0, 50)
$$

Línea 2 NO ES ESTOCÁSTICA

## Priors

Predictive Sampling de las Priors $\alpha$ y $\beta$:

```{r}
set.seed(2971)

N <- 100

alpha <- rnorm(N, 183, 30)
beta <- rnorm(N, 0, 10)
```


```{r}
plot(NULL,
     xlim = range(d2$weight), ylim = c(-100, 400),
     xlab = "weight", ylab="height")
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext( "b~dnorm(0,10)")
xbar <- mean(d2$weight)

for (i in 1:N) curve(alpha[i] + beta[i] * (x - xbar),
                     from = min(d2$weight), to = max(d2$weight), 
                     add = TRUE,
                     col = col.alpha("black", 0.2))
```
The
pattern doesn’t look like any human population at all. It essentially says that the relationship between weight and height could be absurdly positive or negative. Before we’ve even seen
the data, this is a bad model. Can we do better?

$$
\beta \sim \text{Log-Normal}(0, 1)
$$
```{r}
b <- rlnorm(1e4, 0, 1)
dens(b, xlim = c(0, 5), adj = 0.1)
```

Nueva prior-predictive sample:

```{r}
set.seed(2971)

N <- 100

alpha <- rnorm(N, 183, 30)
beta <- rlnorm(N, 0, 10)
```


```{r}
plot(NULL,
     xlim = range(d2$weight), ylim = c(-100, 400),
     xlab = "weight", ylab="height")
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext( "log(b)~dnorm(0, 1)")
xbar <- mean(d2$weight)

for (i in 1:N) curve(alpha[i] + beta[i] * (x - xbar),
                     from = min(d2$weight), to = max(d2$weight), 
                     add = TRUE,
                     col = col.alpha("black", 0.2))
```
To figure out what this prior implies, we have to simulate
the prior predictive distribution. There is no other reliable way to understand.

## Posterior

```{r}
# define the average weight, x-bar
xbar <- mean(d2$weight)

# fit model
m4.3 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - xbar),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ), data=d2)
```

## Interpretación de la posterior

### Tablas de las distribuciones marginales

```{r}
precis(m4.3)
```
```{r}
round(vcov(m4.3), 3)
```
```{r}
pairs(m4.3)
```

### Gráficos posterior - datos

```{r}
plot(height ~ weight, data = d2, col = rangi2)
post <- extract.samples(m4.3)
a_map <- mean(post$a)
b_map <- mean(post$b)
curve(a_map + b_map * (x - xbar), add = TRUE)
```

```{r}
# extract 20 samples from the posterior
post <- extract.samples(m4.3, n = 20)

# displayrawdataandsamplesize
plot(d2$weight, d2$height,
      xlim = range(d2$weight), ylim = range(d2$height),
      col = rangi2, xlab = "weight", ylab = "height")

# plotthelines,withtransparency
for (i in 1:20)
  curve(post$a[i] + post$b[i] * (x - mean(d2$weight)),
         col = col.alpha("black", 0.3), add=TRUE)
```

